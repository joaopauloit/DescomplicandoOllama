# Descomplicando o Ollama
## DAY-17: Descomplicando Ollama

## Conte√∫do do Day-16

- [Descomplicando o Kubernetes](#descomplicando-o-kubernetes)

## O que iremos ver hoje?

Hoje √© dia de entender o que √© o Ollama e como ele pode nos ajudar a rodar modelos de Machine Learning no Kubernetes. Imagine ter o poder de rodar modelos de Machine Learning como Gemma, Llama, Mistral, Code Llama, entre outros, de forma simples e descomplicada. Isso √© o que o Ollama nos proporciona.

### O que s√£o Modelos de Machine Learning?

Um Modelo de IA √© um programa de computador que aprende com os dados. Ele √© treinado para reconhecer padr√µes e tomar decis√µes com base nesses padr√µes. Os modelos de IA s√£o usados em uma variedade de aplica√ß√µes, como reconhecimento de fala, reconhecimento de imagem, tradu√ß√£o de idiomas, diagn√≥stico m√©dico, previs√£o do tempo, entre outros.

Hoje temos alguns desses modelos dispon√≠veis para rodar no Kubernetes, como Gemma, Llama, Mistral, Code Llama, entre outros. E o Ollama √© a ferramenta que nos ajuda a rodar esses modelos de forma simples e descomplicada.

Ent√£o imagina ter um ChatGPT rodando na sua m√°quina ou cluster? Agora imagina n√£o ter somente o ChatGPT, mas tamb√©m outros modelos de Machine Learning super poderosos rodando no mesmo lugar.

Agora vamos entender como funciona o Ollama, a ferramenta que nos ajuda a rodar esses modelos no Kubernetes de uma maneira super simples e r√°pida.

### O que √© o Ollama?

Ollama √© uma ferramenta open-source que permite rodar, criar e compartilhar grandes modelos de linguagem (LLMs) localmente, atrav√©s de uma interface de linha de comando em sistemas operacionais como macOS, Linux e uma vers√£o de pr√©-visualiza√ß√£o para Windows. Oferece suporte a uma ampla gama de modelos, incluindo Llama 2, Code Llama, Mistral, entre outros, permitindo personaliza√ß√£o e cria√ß√£o de modelos pr√≥prios. Para usar o Ollama, √© poss√≠vel baix√°-lo diretamente do site oficial ou utilizar uma imagem Docker dispon√≠vel no Docker Hub.

O Ollama suporta v√°rios modelos com diferentes par√¢metros e tamanhos, permitindo que usu√°rios com pelo menos 8 GB de RAM executem modelos de 7B, 16 GB para modelos de 13B e 32 GB para modelos de 33B. Isso inclui uma ampla variedade de usos, desde modelos espec√≠ficos para chat at√© modelos multimodais que combinam processamento de linguagem natural e vis√£o computacional.

Quando estamos falando sobre modelos de 7B por exemplo, estamos falando de modelos que possuem 7 bilh√µes de par√¢metros. Isso √© muito poderoso e pode ser utilizado para diversas aplica√ß√µes, como ChatGPT, tradu√ß√£o de idiomas, entre outros. Par√¢metros s√£o os valores que o modelo aprende durante o treinamento. Quanto mais par√¢metros, mais complexo e poderoso √© o modelo.

Al√©m disso, o Ollama oferece compatibilidade inicial com a API de Completions de Chat da OpenAI, facilitando o uso de ferramentas existentes constru√≠das para OpenAI com modelos locais atrav√©s do Ollama. Recentemente, tamb√©m introduziu suporte para placas gr√°ficas AMD em Windows e Linux, expandindo suas capacidades de acelera√ß√£o.

Para mais informa√ß√µes sobre como come√ßar a usar o Ollama, modelos suportados e personaliza√ß√£o, voc√™ pode consultar a documenta√ß√£o oficial no [GitHub](https://github.com/ollama/ollama) e no [blog](https://ollama.com/blog) do Ollama.

### Instalando o Ollama

Para instalar o Ollama, voc√™ pode baixar diretamente do site oficial ou utilizar uma imagem Docker dispon√≠vel no Docker Hub. Para instalar o Ollama via Docker, voc√™ pode utilizar o seguinte comando:

```bash
curl -fsSL https://ollama.com/install.sh | bash
```

Lembrando que √© poss√≠vel instalar o Ollama no Windows e MacOS, mas a instala√ß√£o via Docker √© a mais recomendada no Linux por conta do suporte em utilizar placas gr√°ficas do host onde o Docker est√° rodando. De qualque forma, voc√™ pode consultar a documenta√ß√£o oficial para mais informa√ß√µes.

Agora que j√° temos o Ollama instalado, vamos verificar se est√° tudo funcionando corretamente.

```bash
ollama --version
```

Se tudo estiver funcionando corretamente, voc√™ ver√° a vers√£o do Ollama que est√° rodando na sua m√°quina, como por exemplo:

```bash
ollama version is 0.1.29
```

### Rodando um modelo de Machine Learning com o Ollama

Vamos iniciar os nossos estudos rodando um modelo de Machine Learning com o Ollama. Para isso, vamos utilizar o modelo chamado Llama 2, que √© um modelo de 7B de par√¢metros. O Llama 2 √© um modelo de linguagem treinado pela Meta e √© projetado para uma variedade de tarefas de processamento de linguagem natural, incluindo gera√ß√£o de texto, assist√™ncia em programa√ß√£o e chatbots. O Llama 2 vem em v√°rias vers√µes, variando em tamanho de 7 bilh√µes a 70 bilh√µes de par√¢metros, permitindo uma ampla gama de aplica√ß√µes dependendo das necessidades espec√≠ficas de computa√ß√£o e da complexidade da tarefa.

Al√©m disso, o Llama 2 √© not√°vel por sua abordagem quase aberta ao c√≥digo fonte, com algumas restri√ß√µes para uso comercial por grandes organiza√ß√µes e para determinadas finalidades, alinhando-se mais com uma pol√≠tica de "abordagem aberta" do que com o open source tradicional sob a defini√ß√£o da Open Source Initiative. Estas restri√ß√µes visam encorajar um uso respons√°vel da IA enquanto mant√©m certa flexibilidade para inova√ß√£o e desenvolvimento‚Äã‚Äã.

Dito isso, bora rodar o Llama 2 com o Ollama. Para isso, voc√™ pode utilizar o seguinte comando:

```bash
ollama run llama-2
```

Na primeira vez que voc√™ rodar o comando, o Ollama ir√° baixar o modelo do Llama 2 e depois ir√° rodar o modelo. Isso pode demorar um pouco, ent√£o tenha paci√™ncia. Depois que o modelo estiver rodando, voc√™ poder√° interagir com ele.

```bash
pulling manifest
pulling 8934d96d3f08... 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 3.8 GB
pulling 8c17c2ebb0ea... 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 7.0 KB
pulling 7c23fb36d801... 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 4.8 KB
pulling 2e0493f67d0c... 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   59 B
pulling fa304d675061... 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   91 B
pulling 42ba7f8a01dd... 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  557 B
verifying sha256 digest
writing manifest
removing any unused layers
success
>>> Send a message (/? for help):
```

Quando voc√™ ver o prompt `>>> Send a message (/? for help):`, voc√™ poder√° interagir com o modelo. Voc√™ pode enviar uma mensagem para o modelo e ele ir√° responder com base no que voc√™ enviou. Por exemplo, voc√™ pode enviar uma mensagem como `Hello, how are you?` e o modelo ir√° responder com base no que ele aprendeu durante o treinamento.

```bash
>>> Send a message (/? for help): Ol√°, tudo tranquilo por a√≠?
```

E o modelo ir√° responder com algo como:

```bash
Obrigado por perguntar! üòä Ent√£o, tudo est√° tranquilo por aqui. Estou aqui para ajudar com quaisquer suas
d√∫vidas ou problemas, ent√£o n√£o hesite em me consultar. Qual √© o assunto? ü§î
```

Pronto, j√° estamos com o Llama 2 rodando na nossa m√°quina. Agora voc√™ pode interagir com o modelo e ver o que ele √© capaz de fazer. Al√©m disso, voc√™ pode rodar outros modelos para ir testando e estudando.

Para sair do modelo, voc√™ pode utilizar o comando `Ctrl + d` no terminal ou digitar `/bye` no prompt do modelo. Simples como voar!

Agora, para que voc√™ possa listar quais s√£o os modelos dispon√≠veis para rodar com o Ollama, voc√™ pode utilizar o seguinte comando:

```bash
ollama list
```

Isso ir√° listar todos os modelos dispon√≠veis na sua m√°quina, ou seja, que voc√™ j√° baixou. Fique atento pois os modelos podem ser grandes e ocupar muito espa√ßo no seu disco, quando mais par√¢metros, maior o tamanho do modelo.

Vamos baixar mais um modelo, agora √© a vez de um focado em nos ajudar com c√≥digo. Vamos baixar o modelo Code Llama, que √© um modelo de linguagem treinado pela Meta e √© projetado para uma variedade de tarefas de processamento de linguagem natural focado em ajudar a tornar os fluxos de trabalho de desenvolvimento de software mais eficientes. O Code Llama vem em v√°rias vers√µes, variando em tamanho de 7 bilh√µes a 70 bilh√µes de par√¢metros, permitindo uma ampla gama de aplica√ß√µes dependendo das necessidades espec√≠ficas de computa√ß√£o e da complexidade da tarefa. O Code Llama ser√° √∫til para quem trabalha com desenvolvimento de software e precisa de ajuda com c√≥digo, ou seja, todos n√≥s! hahahah

Para baixar o modelo Code Llama, voc√™ pode utilizar o seguinte comando:

```bash
ollama pull code-llama
```

Agora somente baixamos o modelo, para rodar o modelo Code Llama, voc√™ pode utilizar o seguinte comando:

```bash
ollama run code-llama

pulling manifest
pulling 3a43f93b78ec... 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 3.8 GB
pulling 8c17c2ebb0ea... 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 7.0 KB
pulling 590d74a5569b... 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 4.8 KB
pulling 2e0493f67d0c... 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   59 B
pulling 7f6a57943a88... 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  120 B
pulling 316526ac7323... 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  529 B
verifying sha256 digest
writing manifest
removing any unused layers
success
```

E pronto, voc√™ j√° est√° rodando o modelo Code Llama na sua m√°quina. Agora voc√™ pode interagir com o modelo e ver o que ele √© capaz de fazer. Se divirta!

Vamos listar os nosso modelos dispon√≠veis para rodar com o Ollama:

```bash
ollama list
```

A sa√≠da ser√° algo como:

```bash
NAME            	ID          	SIZE  	MODIFIED
codellama:latest	8fdf8f752f6e	3.8 GB	17 seconds ago	
llama2:latest   	78e26419b446	3.8 GB	9 minutes ago 	
```

Caso voc√™ queira remover um modelo que voc√™ baixou, voc√™ pode utilizar o seguinte comando:

```bash
ollama remove llama-2
```

Pronto, um modelo a menos na sua m√°quina. :D

### Instalando o Open WebUI

Para que possamos ter uma interface gr√°fica para a nossa IA, temos algumas op√ß√µes. Por√©m com toda certeza uma se destaca pela facilidade de uso, funcionalidade e pela comunidade, essa ferramenta √© o Open WebUI.

Ela suporta diversos plataformas de LLMs como o Ollama e as API da OpenAI.

Durante esse artigo, irei mostrar como instalar e utilizar o Open WebUI com o Ollama utilizando o Docker, portanto voc√™ precisar√° ter o Docker instalado na sua m√°quina.

Para instalar o Open WebUI, voc√™ pode utilizar o seguinte comando:

```bash
docker run -d --network=host -v open-webui:/app/backend/data -e OLLAMA_BASE_URL=http://127.0.0.1:11434 --name open-webui --restart always ghcr.io/open-webui/open-webui:main
```

Onde:
- `--network=host` √© utilizado para que o container tenha acesso a rede da m√°quina host.
- `-v open-webui:/app/backend/data` √© utilizado para criar um volume para armazenar os dados do Open WebUI.
- `-e OLLAMA_BASE_URL` √© utilizado para informar a URL base do Ollama, que est√° rodando na sua m√°quina, se voc√™ estiver rodando o Ollama em outro lugar, voc√™ pode alterar o IP e a porta.
- `--name open-webui` √© utilizado para dar um nome ao container.
- `--restart always` √© utilizado para que o container seja reiniciado sempre que a m√°quina for reiniciada.
- `ghcr.io/open-webui/open-webui:main` √© a imagem do Open WebUI que ser√° utilizada.

Simples como voar e do neida voc√™ j√° tem uma interface gr√°fica para interagir com a sua IA. Para acessar o Open WebUI, voc√™ pode usar o seu navegador e acessar o endere√ßo `http://localhost:8080`.

Voc√™ precisar√° criar um user, mas n√£o se preocupe, √© um usu√°rio local para acessar a interface gr√°fica. Depois de criar o usu√°rio, voc√™ poder√° acessar a interface gr√°fica e interagir com a sua IA, de uma forma muito mais amig√°vel e visual, bem parecida com o que voc√™ j√° viu no ChatGPT da OpenAI.

![Usando o Open WebUI para acessar a nossa IA usando Ollama](images/image.png)


### Instalando o Ollama no Kubernetes

Agora que j√° sabemos como o Ollama funciona, bora entender como fazer com que ele funcione no Kubernetes de uma maneira simples e descomplicada, como √© a nossa proposta aqui no Descomplicando.

Eu estou utilizando o Minikube para esse exemplo, caso voc√™ queira saber como eu instalei e o quais os comandos que eu utilizei, vou colar aqui para voc√™:

```bash
minikube start --driver docker --container-runtime docker --gpus all
```

Onde:
- `--driver docker` √© utilizado para que o Minikube utilize o Docker como driver.
- `--container-runtime docker` √© utilizado para que o Minikube utilize o Docker como container runtime.
- `--gpus all` √© utilizado para que o Minikube utilize todas as GPUs dispon√≠veis no host.
  
Super simples! Lembre-se sempre de ativar as GPUs no Minikube para que voc√™ possa utilizar as GPUs no Kubernetes.

Ahhh, eu tamb√©m j√° adicionei o Ingress Controller, no caso o Nginx Ingress Controller. Para instala-lo no Minikube √© super simples:

```bash
minikube addons enable ingress
```

Agora sim! J√° temos o Minikube rodando com o Docker como driver, com todas as GPUs dispon√≠veis e com o Ingress Controller habilitado. \o/

Uma coisa que me deixa super feliz √© a exist√™ncia do [Helm](https://helm.sh/), que √© um gerenciador de pacotes para Kubernetes. Com o Helm, podemos instalar aplica√ß√µes no Kubernetes de uma maneira super simples! E o melhor, o Ollama j√° possui um [chart Helm](https://artifacthub.io/packages/helm/ollama-helm/ollama) dispon√≠vel para que possamos instalar o Ollama no Kubernetes.

Conhe√ßa todos os detalhes e op√ß√µes dispon√≠veis no [chart Helm do Ollama](https://artifacthub.io/packages/helm/ollama-helm/ollama).

Para o nosso exemplo, vamos utilizar o chart Helm do Ollama para instalar o Ollama no Kubernetes. Para isso, voc√™ precisa ter o Helm instalado na sua m√°quina. Se voc√™ ainda n√£o tem o Helm instalado, voc√™ pode seguir o [guia de instala√ß√£o do Helm](https://helm.sh/docs/intro/install/).

Agora com o Helm instalado, vamos adicionar o reposit√≥rio do Ollama no Helm para que possamos instalar o Ollama no Kubernetes. 

Antes de mais nada, precisamos criar um namespace para o Ollama:

```bash
kubectl create namespace ollama
```

Para adicionar o reposit√≥rio do Ollama, voc√™ pode utilizar o seguinte comando:

```bash
helm repo add ollama-helm https://otwld.github.io/ollama-helm/
helm repo update
```

Com isso j√° temos o reposit√≥rio do Ollama adicionado no Helm e atualizado.

Agora precisamos instalar o Ollama, mas antes vamos criar o nosso arquivo `values.yaml` para configurar o Ollama, mas aqui √© somente uma base para voc√™ come√ßar, voc√™ pode alterar conforme a sua necessidade. E por isso √© importante voc√™ consultar a documenta√ß√£o oficial do [chart Helm do Ollama](https://artifacthub.io/packages/helm/ollama-helm/ollama) para saber todas as op√ß√µes dispon√≠veis.

```yaml
ollama:
  gpu:
    # -- Enable GPU integration
    enabled: true

    # -- GPU type: 'nvidia' or 'amd'
    type: 'nvidia'

    # -- Specify the number of GPU to 2
    number: 1

  # -- List of models to pull at container startup
  models:
    - llama2

ingress:
  enabled: true
  hosts:
    - host: ollama.badtux.io
      paths:
        - path: /
          pathType: Prefix
```

No meu caso, eu somente tenho um node com GPU, ent√£o eu configurei o Ollama para utilizar a GPU, mas voc√™ pode alterar conforme a sua necessidade. Al√©m disso, eu configurei para baixar o modelo Llama 2 na inicializa√ß√£o do container e tamb√©m configurei o Ingress para acessar o Ollama pelo endere√ßo `ollama.badtux.io`.

Caso seu cluster Kubernetes n√£o tenha suporte a GPU, voc√™ pode desabilitar a GPU no Ollama, basta alterar o valor de `enabled` para `false`.

Ou ainda, caso o seu cluster tenha mais de uma GPU, voc√™ pode alterar o valor de `number` para a quantidade de GPU que voc√™ deseja utilizar.

Ahhh, eu j√° tenho o DNS configurado para o endere√ßo `ollama.badtux.io`, apontando para o IP do meu Ingress Controller, ent√£o voc√™ precisa alterar o valor de `host` para o endere√ßo que voc√™ deseja utilizar para acessar o Ollama.

Inclusive, caso voc√™ esteja executando o Kubernetes em um ambiente local, voc√™ pode adicionar o endere√ßo no seu arquivo `/etc/hosts` para que voc√™ possa acessar o Ollama pelo endere√ßo configurado no Ingress.

No caso do Minikube, basta digitar o seguinte comando:

```bash
echo "$(minikube ip) ollama.badtux.io" | sudo tee -a /etc/hosts
```

Assim j√° pegamos o IP do Minikube, que √© utilizado pelo Ingress Controller, e adicionamos no arquivo `/etc/hosts` para que possamos acessar o Ollama pelo endere√ßo `ollama.badtux.io`. Simples como voar! :D

Dito isso, agora vamos instalar o Ollama no Kubernetes com o Helm. Para isso, voc√™ pode utilizar o seguinte comando:

```bash
helm install ollama ollama-helm/ollama -n ollama -f values.yaml
```

A sa√≠da ser√° algo como:

```bash
NAME: ollama
LAST DEPLOYED: Wed Mar 27 20:13:29 2024
NAMESPACE: ollama
STATUS: deployed
REVISION: 1
NOTES:
1. Get the application URL by running these commands:
  http://ollama.badtux.io/
```

Pronto, o Ollama foi instalado no Kubernetes, mas ainda temos que esperar os pods ficarem prontos para acessar o Ollama. Para verificar o status dos pods, voc√™ pode utilizar o seguinte comando:

```bash
kubectl get pods -n ollama
```

Dependendo da quantidade e do tamanho dos modelos que voc√™ configurou para baixar, pode demorar um pouco para os pods ficarem prontos. Ent√£o tenha paci√™ncia.

Quando os pods estiverem prontos, voc√™ poder√° acessar o Ollama pelo endere√ßo que voc√™ configurou no Ingress. No meu caso, eu posso acessar o Ollama pelo endere√ßo `http://ollama.badtux.io/`.

Vamos fazer um `curl` para verificar se o Ollama est√° funcionando corretamente:

```bash
curl http://ollama.badtux.io/
```

A sa√≠da ser√° algo como:

```bash
Ollama is running%
``` 

Pronto, o nosso Ollama est√° pronto!

Agora vamos setar a vari√°vel de ambiente `OLLAMA_HOST` para que o CLI do Ollama possa interagir com o Ollama que est√° no Kubernetes.

```bash
export OLLAMA_HOST=http://ollama.badtux.io/
```

Agora voc√™ j√° pode usar o CLI do Ollama para interagir com o Ollama que est√° em execu√ß√£o no Kubernetes.

```bash
ollama run llama-2
```

Simples como voar! Agora o nosso Ollama est√° rodando no Kubernetes! :D

### Instalando o Open WebUI no Kubernetes

O Open WebUI √© uma ferramenta super poderosa que nos ajuda a ter uma interface gr√°fica para interagir com a nossa IA. E o melhor, o Open WebUI j√° possui um [chart Helm](https://github.com/open-webui/open-webui/tree/main/kubernetes/helm), por√©m n√≥s vamos optar por instalar o Open WebUI no Kubernetes atrav√©s dos manifestos YAML dispon√≠veis no reposit√≥rio do Open WebUI.

N√£o iremos utilizar o chart Helm do Open WebUI, porqu√™ com o chart ter√≠amos que deployar o Ollama utilizando o mesmo chart, pois n√£o tem como desabilitar o Ollama no chart do Open WebUI. Eu vou criar o nosso Helm Chart no futuro, ou ainda, acabar utilizando o Helm Chart do Open WebUI, mas por enquanto vamos utilizar os manifestos YAML dispon√≠veis no reposit√≥rio do Open WebUI. Ou quem sabe ainda, abrir um PR para adicionar a op√ß√£o de desabilitar o Ollama no chart do Open WebUI e/ou habilitar o Open WebUI no chart do Ollama. hahahaha (me desculpe caso j√° exista essa op√ß√£o e eu n√£o tenha visto).

Vamos criar o nosso arquivo `open-webui.yaml` para criar o Namespace para o Open WebUI:

```yaml
apiVersion: v1
kind: Namespace
metadata:
  name: open-webui
```

Agora vamos criar o nosso arquivo `webui-deployment.yaml` para criar o Deployment do Open WebUI:

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: open-webui-deployment
  namespace: open-webui
spec:
  replicas: 1
  selector:
    matchLabels:
      app: open-webui
  template:
    metadata:
      labels:
        app: open-webui
    spec:
      containers:
      - name: open-webui
        image: ghcr.io/open-webui/open-webui:main
        ports:
        - containerPort: 8080
        resources:
          requests:
            cpu: "500m"
            memory: "500Mi"
          limits:
            cpu: "1000m"
            memory: "1Gi"
        env:
        - name: OLLAMA_BASE_URL
          value: "http://ollama.badtux.io/"
        tty: true
        volumeMounts:
        - name: webui-volume
          mountPath: /app/backend/data
      volumes:
      - name: webui-volume
        persistentVolumeClaim:
          claimName: ollama-webui-pvc
```

Eu somente modifiquei o valor de `OLLAMA_BASE_URL` para o endere√ßo que eu configurei no Ingress do Ollama, no caso `http://ollama.badtux.io/`.

Fique a vontade para alterar conforme a sua necessidade, inclusive configura√ß√µes de recursos, volumes, etc.

Com o Deployment criado, precisamos de um Service e na sequ√™ncia um Ingress para que possamos acessar o Open WebUI.

Para o Service, vamos criar o arquivo `webui-service.yaml`:

```yaml
apiVersion: v1
kind: Service
metadata:
  name: open-webui-service
  namespace: open-webui
spec:
  type: NodePort  # Use LoadBalancer if you're on a cloud that supports it
  selector:
    app: open-webui
  ports:
    - protocol: TCP
      port: 8080
      targetPort: 8080
      # If using NodePort, you can optionally specify the nodePort:
      # nodePort: 30000
```

E para o Ingress, vamos criar o arquivo `webui-ingress.yaml`:

```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: open-webui-ingress
  namespace: open-webui
  #annotations:
    # Use appropriate annotations for your Ingress controller, e.g., for NGINX:
    # nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  rules:
  - host: open-webui.badtux.io
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: open-webui-service
            port:
              number: 8080
```

Lembrando que voc√™ precisa alterar o valor de `host` para o endere√ßo que voc√™ deseja utilizar para acessar o Open WebUI. E caso voc√™ esteja utilizando o Minikube, voc√™ pode adicionar o endere√ßo no seu arquivo `/etc/hosts` para que voc√™ possa acessar o Open WebUI pelo endere√ßo configurado no Ingress, como eu j√° falei um zilh√£o de vezes. hahahah

Agora, para finalizar, vamos criar o arquivo `webui-pvc.yaml` para criar o PersistentVolumeClaim para o Open WebUI:

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  labels:
    app: ollama-webui
  name: ollama-webui-pvc
  namespace: open-webui
spec:
  accessModes: ["ReadWriteOnce"]
  resources:
    requests:
      storage: 2Gi
```

Agora sim hein, todos os arquivos que precisamos para o nosso show funcionar est√£o prontos! Agora vamos aplicar esses arquivos e torcer para que n√£o tenhamos digitados nada errado. :D

```bash
kubectl apply -f open-webui.yaml
kubectl apply -f webui-deployment.yaml
kubectl apply -f webui-service.yaml
kubectl apply -f webui-ingress.yaml
kubectl apply -f webui-pvc.yaml
```

Lembrando que esse √© exatamente o conte√∫do do arquivo dispon√≠vel no reposit√≥rio do Open WebUI. Clique [aqui](https://github.com/open-webui/open-webui/tree/main/kubernetes/manifest/base) para acessar o reposit√≥rio.

Vamos esperar os pods ficarem prontos para acessar o Open WebUI. Para verificar o status dos pods, voc√™ pode utilizar o seguinte comando:

```bash
kubectl get pods -n open-webui
```

Quando tudo estiver ok, j√° podemos acessar o Open WebUI atrav√©s do endere√ßo que configuramos no Ingress. No meu caso, eu posso acessar o Open WebUI pelo endere√ßo `http://open-webui.badtux.io/`.

![Sign in to Open WebUI](images/image-1.png)

Como ainda n√£o temos um usu√°rio criado, voc√™ precisa criar um usu√°rio para acessar o Open WebUI. Depois de criar o usu√°rio, voc√™ poder√° acessar a interface gr√°fica e interagir com a sua IA de uma forma muito mais amig√°vel e visual. Os dados s√£o armazenados no PersistentVolumeClaim que criamos, e n√£o √© compartilhado com ning√∫em, ent√£o fique tranquilo.

Depois de criado a sua conta, voc√™ poder√° acessar o Open WebUI e enviar mensagens para a sua IA, ela ir√° responder com base no que ela aprendeu durante o treinamento e de acordo com o modelo que voc√™ est√° utilizando.

![Primeiro login Open WebUI](images/image-2.png)

Na parte superior da tela, voc√™ pode ver o nome do modelo que voc√™ est√° utilizando, n√≥s estamos utilizando o modelo Llama2, que definimos l√° no arquivo `values.yaml` do Ollama. Se precisar de outros, voc√™ pode adicionar na lista de modelos no chart Helm do Ollama e depois fazer o `helm upgrade` para atualizar o Ollama no Kubernetes. √â bem simples!

Acho que o nosso servi√ßo est√° completo, por enquanto! Vamos ver o que podemos aprontar na parte 3 dessa s√©rie! :D

Para acessar o Parte 1, clique [aqui](https://linuxtips.io/blog/descomplicando-ollama-parte-1).
